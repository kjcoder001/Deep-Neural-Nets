{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import download,europarl\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU,Input,Dense,Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "# The dataset used is taken from europarl,which consists of translations in different pairs of languages spoken at the\n",
    "# european union parliament.\n",
    "\n",
    "language_code = 'fr' # french --> English\n",
    "\n",
    "europarl.maybe_download_and_extract(language_code=language_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the source(french) and destination(english) corpus\n",
    "\n",
    "mark_start = 'ssss '\n",
    "mark_end = ' eeee'\n",
    "source_data = europarl.load_data(english=False,language_code=language_code)\n",
    "dest_data = europarl.load_data(english=True,language_code=language_code,start=mark_start,end=mark_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reprise de la session'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "source_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss Resumption of the session eeee'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerPlus(Tokenizer):\n",
    "    '''Adding some more functionality to the Tokenizer class.'''\n",
    "    \n",
    "    def __init__(self,texts,padding,reverse=False,number_of_words=10000):\n",
    "        '''\n",
    "        parameters:\n",
    "        texts - input data(source text)\n",
    "        padding - a string as 'pre' or 'post' which tells the postion of padding/truncating\n",
    "        reverse - whether a list of integer tokens has to be reversed for better model predictions(helps the model learn \n",
    "                  short term dependencies better)\n",
    "        number_of_words - size of chosen dictionary\n",
    "        '''\n",
    "        \n",
    "        Tokenizer.__init__(self,10000)\n",
    "        # fits to generate a 10,000 word vocabulary.The indices are given based on word's frequnecy in the corpus\n",
    "        self.fit_on_texts(texts)\n",
    "        # maps indices back to words\n",
    "        self.index_to_word = dict(zip(self.word_index.values(),self.word_index.keys()))\n",
    "        # maps all sequences in the data to list of tokens where each word in a sequence corresponds to a word.\n",
    "        # So, tokens is a list of lists.\n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "        \n",
    "        if reverse:\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "        \n",
    "        # calculating the maximum length of sequences we can consider for  \n",
    "        self.number_of_tokens = [len(x) for x in self.tokens]\n",
    "        self.max_tokens = int(np.mean(self.number_of_tokens) + 2*np.std(self.number_of_tokens))\n",
    "        \n",
    "        self.padded_tokens = pad_sequences(self.tokens,self.max_tokens,padding=padding,truncating=truncating)\n",
    "    \n",
    "    def token_to_word(self,token):\n",
    "        ''' returns a word corresponding to the token'''\n",
    "        return \" \" if token==0 else self.index_to_word[token]\n",
    "    \n",
    "    def tokens_to_string(self,tokens):\n",
    "        ''' combines the words to create a sequence for a list of tokens'''\n",
    "        return ' '.join([self.index_to_word[x] for x in tokens if x!=0])\n",
    "    \n",
    "    def text_to_tokens(self,text,reverse=False,padding=False):\n",
    "        token_list_for_text = np.array(self.texts_to_sequences([text]))\n",
    "        if reverse:\n",
    "            token_list_for_text = np.flip(token_list_for_text,axis=-1)\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "        \n",
    "        if padding:\n",
    "            token_list_for_text = pad_sequences(token_list_for_text,\n",
    "                                   maxlen=self.max_tokens,\n",
    "                                   padding='pre',\n",
    "                                   truncating=truncating)\n",
    "\n",
    "        return token_list_for_text        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing tokenizers for source and destination languages\n",
    "source_tokenizer = TokenizerPlus(texts=source_data,padding='pre',reverse=True,number_of_words=10000)\n",
    "dest_tokenizer = TokenizerPlus(texts=dest_data,padding='post',reverse=False,number_of_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for storing padded tokens of source and destination.The padded tokens are a 2-d np array,where each array has \n",
    "# tokens for a sequence present in the corpus.\n",
    "source_tokens = source_tokenizer.padded_tokens\n",
    "dest_tokens = dest_tokenizer.padded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Tokens for start and end of texts,i.e integers that represent start and end of text.\n",
    "start_index = dest_tokenizer.word_index[mark_start.strip()]\n",
    "print(start_index)\n",
    "end_index = dest_tokenizer.word_index[mark_end.strip()]\n",
    "print(end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0, 5571, 1058,    1,  634,  366,   41,    8, 8073,    9,\n",
       "        278,   67,   41,   17,    3,  385,  923, 1810, 3580,    4, 5129,\n",
       "         45,  410,   14,   56,   42,   11, 1199,    2, 1653, 2430,   17])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples\n",
    "\n",
    "# Sample tokens from source language\n",
    "source_tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vacances bonnes de passé avez vous que espérant en mes tous vous je et dernier décembre 17 vendredi le interrompue été avait qui européen parlement du session la reprise déclare je'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping the tokens back to words\n",
    "source_tokenizer.tokens_to_string(source_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the actual data corresponding to the tokens.Clearly the 2 strings are reversed\n",
    "source_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,   13, 2742, 2976,    1, 1131,    4,    1,   24,   50, 4822,\n",
       "         15, 3308, 1830,  904,  985,    6,   13,   31,   58,  339,  254,\n",
       "          5,  298,   43,    9, 1299,   78,  162,    7,    1,  194,    8,\n",
       "         43, 3799,    9, 6847,  492,    3,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss i declare resumed the session of the european parliament adjourned on friday 17 december 1999 and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant period eeee'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_tokenizer.tokens_to_string(dest_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period. eeee'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The destination language tokens have not been reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The architecture for machine translation involves 2 RNN, one that encodes the source language into some meaningful thought\n",
    "# vecot that RNN can understand, and a decoder which decodes the input from encoder.The way it works is that during training\n",
    "# the decoder is given the corresponding translation of the source language and the decoder output is same as the decoder\n",
    "# input , just shifted one time-step ahead so that the nn can learn the correct mappings in a supervised way.\n",
    "\n",
    "encoder_input_data = source_tokens\n",
    "decoder_input_data = dest_tokens[:,:-1]\n",
    "decoder_output_data = dest_tokens[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,   13, 2742, 2976,    1, 1131,    4,    1,   24,   50, 4822,\n",
       "         15, 3308, 1830,  904,  985,    6,   13,   31,   58,  339,  254,\n",
       "          5,  298,   43,    9, 1299,   78,  162,    7,    1,  194,    8,\n",
       "         43, 3799,    9, 6847,  492,    3,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  13, 2742, 2976,    1, 1131,    4,    1,   24,   50, 4822,   15,\n",
       "       3308, 1830,  904,  985,    6,   13,   31,   58,  339,  254,    5,\n",
       "        298,   43,    9, 1299,   78,  162,    7,    1,  194,    8,   43,\n",
       "       3799,    9, 6847,  492,    3,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data[1] # shifted one-time step from input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "# Instead of using the built in keras models like Sequential,we'll instead create the layer objects and connect them \n",
    "# manually later.This gives us more freedom and leverage to customize our model.\n",
    "\n",
    "encoder_input = Input(shape=(None,),name='encoder_input')\n",
    "encoder_embedding = Embedding(input_dim=10000,output_dim=128,name='encoder_embedding')\n",
    "units = 512\n",
    "encoder_gru1 = GRU(units,name='encoder_gru1',return_sequences=True)\n",
    "encoder_gru2 = GRU(units,name='encoder_gru2',return_sequences=True)\n",
    "encoder_gru3 = GRU(units,name='encoder_gru3',return_sequences=False)\n",
    "\n",
    "#connect the layers\n",
    "\n",
    "layer = encoder_input\n",
    "layer = encoder_embedding(layer)\n",
    "layer = encoder_gru1(layer)\n",
    "layer = encoder_gru2(layer)\n",
    "layer = encoder_gru3(layer)\n",
    "\n",
    "encoder_output = layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "state_size = 512\n",
    "decoder_initial = Input(shape=(state_size,), name='decoder_initial')\n",
    "decoder_input = Input(shape=(None,),name='decoder_input')\n",
    "decoder_embedding = Embedding(input_dim=10000,output_dim=128,name='decoder_embedding')\n",
    "\n",
    "decoder_gru1 = GRU(state_size,name='decoder_gru1',return_sequences=True)\n",
    "decoder_gru2 = GRU(state_size,name='decoder_gru2',return_sequences=True)\n",
    "decoder_gru3 = GRU(state_size,name='decoder_gru3',return_sequences=True)\n",
    "\n",
    "decoder_dense = Dense(10000,activation='linear',name='decoder_output')\n",
    "\n",
    "# connect the layers\n",
    "def connect_decoder(initial_state):\n",
    "    \n",
    "    layer_d = decoder_input\n",
    "    layer_d = decoder_embedding(layer_d)\n",
    "    layer_d = decoder_gru1(layer_d,initial_state=initial_state)\n",
    "    layer_d = decoder_gru2(layer_d,initial_state=initial_state)\n",
    "    layer_d = decoder_gru3(layer_d,initial_state=initial_state)\n",
    "    decoder_output = decoder_dense(layer_d)\n",
    "    return decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the models\n",
    "# To realise this complex architecture, we'll build 3 models.\n",
    "#1. A training model which connects the encoder and decoder end to end.Inputs will be the encoder input and decoder input\n",
    "# and output will be decoder output.In short the inputs the decoder will be connected to the output of the encoder.\n",
    "#2. A separate model for encoder which outputs the thought-vector,summary of the input language sequence.\n",
    "#3. A separate model for the decoder.\n",
    "\n",
    "#1. input of encoder is fed into the decoder\n",
    "decoder_output = connect_decoder(initial_state=encoder_output)\n",
    "training_model = Model(inputs=[encoder_input,decoder_input],outputs=[decoder_output])\n",
    "\n",
    "#2\n",
    "encoder_model = Model(inputs=[encoder_input],outputs=[encoder_output])\n",
    "\n",
    "#3\n",
    "decoder_output = connect_decoder(initial_state=decoder_initial)\n",
    "decoder_model = Model(inputs=[decoder_input,decoder_initial],outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(labels,predictions):\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=predictions,labels=labels))\n",
    "    return loss\n",
    "\n",
    "optimizer = RMSprop(lr=0.001)\n",
    "decoder_target = tf.placeholder(dtype='int32',shape=(None,None))\n",
    "training_model.compile(optimizer=optimizer,loss=compute_loss,target_tensors=[decoder_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks to facilitate evaluation\n",
    "path_checkpoint = 'machine_trans_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,monitor='val_loss',verbose=1,save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss',patience=3, verbose=1)\n",
    "callback_tensorboard = TensorBoard(log_dir='./machine_trans_logs/',histogram_freq=0,write_graph=False)\n",
    "callbacks = [callback_early_stopping,callback_checkpoint,callback_tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = {'encoder_input':encoder_input_data,'decoder_input':decoder_input_data}\n",
    "y_data = {'decoder_output':decoder_output_data}\n",
    "validation_split = 10000 / len(encoder_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_model.fit(x=x_data,y=y_data,batch_size=640,epochs=10,validation_split=validation_split,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_text, true_output_text=None):\n",
    "    \"\"\"Translate a single text-string.\"\"\"\n",
    "\n",
    "    \n",
    "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
    "                                                reverse=True,\n",
    "                                                padding=True)\n",
    "    \n",
    "    initial_state = model_encoder.predict(input_tokens)\n",
    "    max_tokens = tokenizer_dest.max_tokens\n",
    "    shape = (1, max_tokens)\n",
    "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
    "    token_int = token_start\n",
    "    output_text = ''\n",
    "    count_tokens = 0\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        \n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "\n",
    "        \n",
    "        x_data = \\\n",
    "        {\n",
    "            'decoder_initial_state': initial_state,\n",
    "            'decoder_input': decoder_input_data\n",
    "        }\n",
    "      \n",
    "        decoder_output = model_decoder.predict(x_data)\n",
    "        token_onehot = decoder_output[0, count_tokens, :]\n",
    "        token_int = np.argmax(token_onehot)\n",
    "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
    "        output_text += \" \" + sampled_word    \n",
    "        count_tokens += 1\n",
    "        \n",
    "    output_tokens = decoder_input_data[0]\n",
    "    \n",
    "    print(\"Input text:\")\n",
    "    print(input_text)\n",
    "    print()\n",
    "\n",
    " \n",
    "    print(\"Translated text:\")\n",
    "    print(output_text)\n",
    "    print()\n",
    "\n",
    "   \n",
    "    if true_output_text is not None:\n",
    "        print(\"True output text:\")\n",
    "        print(true_output_text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
